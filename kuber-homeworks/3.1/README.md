## Домашнее задание

https://github.com/netology-code/kuber-homeworks/blob/main/3.1/3.1.md

## Расчет ресурсов Worker nodes

При расчете ресурсов для Kubernetes кластера важно учитывать не только ресурсы, необходимые для работы приложений, но и дополнительные ресурсы для поддержания работы самого кластера и его сервисов. Эти служебные ресурсы включают такие элементы, как управление нодами, сетевыми компонентами, мониторинг, логирование и другие системные сервисы, которые обеспечивают функционирование кластера.

### Расчёт необходимых ресурсов для проекта

Для развертывания приложения с базой данных, системой кеширования, фронтендом и бекендом необходимо рассчитать общие ресурсы, которые будут потребляться всеми компонентами.

#### 1. Расчет суммарных ресурсов для приложений:

ОЗУ:
```
База данных: 3 × 4 ГБ = 12 ГБ
Кеш: 3 × 4 ГБ = 12 ГБ
Фронтенд: 5 × 0.05 ГБ = 0.25 ГБ
Бекенд: 10 × 0.6 ГБ = 6 ГБ
Итого ОЗУ: 12 + 12 + 0.25 + 6 = 30.25 ГБ
```

Процессорные ядра:
```
База данных: 3 × 1 ядро = 3 ядра
Кеш: 3 × 1 ядро = 3 ядра
Фронтенд: 5 × 0.2 ядра = 1 ядро
Бекенд: 10 × 1 = 10 ядер
Итого процессорные ядра: 3 + 3 + 1 + 10 = 17 ядер
```

#### 2. Добавление служебных ресурсов:

Служебные ресурсы включают системные демоны, сетевые плагины, мониторинг и логирование. Эти ресурсы зависят от типа и размера кластера. В среднем, рекомендуется добавить около 10-20% от общего количества ресурсов для учета служебных потребностей.
```
Дополнительная ОЗУ: 30.25 ГБ × 0.20 = 6.05 ГБ
Дополнительные ядра: 17 ядер × 0.20 = 3.4 ядра
```

#### 3. Учет отказоустойчивости:

Для обеспечения отказоустойчивости и распределения нагрузки рекомендуется использовать хотя бы 3 ноды, чтобы можно было обеспечить перераспределение нагрузки при выходе одной из них из строя. Каждая нода должна иметь ресурсы, достаточные для покрытия своих задач и учета служебных потребностей.

*Принцип расчета ресурсов с учетом отказоустойчивости:*

Если у нас есть кластер из N нод, и мы хотим обеспечить отказоустойчивость на случай выхода из строя одной из них, оставшиеся N−1 ноды должны быть способны взять на себя всю нагрузку. Это означает, что каждая нода должна иметь достаточные ресурсы, чтобы в случае выхода из строя одной ноды нагрузка перераспределилась между оставшимися N−1 нодами.

*Почему умножаем на N/(N−1)​*
```
Рассмотрим конкретный пример:

    Исходное количество нод: 3
    Количество нод после выхода одной из них из строя: 3 − 1 = 2
```

Чтобы нагрузка распределялась равномерно после отказа одной ноды, каждая из оставшихся нод должна взять на себя больше нагрузки. В идеале это увеличение должно компенсировать потерю одной ноды, что означает, что ресурсы на каждую оставшуюся ноду должны быть рассчитаны так, чтобы каждая нода могла обслужить больше ресурсов. Формула для этого расчета:
```
Нагрузка на каждую ноду = Общая нагрузка ​/ Количество оставшихся нод
```

Таким образом, если у нас есть 3 ноды и одна выходит из строя, нам нужно пересчитать ресурсы так, чтобы две оставшиеся ноды могли справиться с нагрузкой:
```
Ресурсы для каждой ноды = Общие ресурсы / Количество оставшихся нод
```

Это означает, что каждая нода должна обладать ресурсами, достаточными для поддержания работы, которые обычно распределяются на три ноды, но теперь будут распределяться на две.

Для учета выхода одной ноды из строя, увеличиваем ресурсы каждой ноды на коэффициент:
```
Коэффициент увеличения = N/(N−1)​ = 3/2​ = 1.5
```

Итак, умножение ресурсов на 1.5 означает, что каждая из оставшихся двух нод должна иметь ресурсы, достаточные для выполнения 1.5 частей нагрузки, которые обычно распределялись на три ноды. Этот коэффициент гарантирует, что две оставшиеся ноды смогут справиться с нагрузкой, если одна нода выйдет из строя.

#### 4. Расчет итоговых ресурсов с учетом запаса:

Теперь умножаем общие ресурсы на коэффициент для учета выхода одной ноды из строя:
```
ОЗУ с запасом: (30.25 ГБ + 6.05 ГБ) × 1.5 = 36.3 ГБ × 1.5 = 54.45 ГБ
Процессорные ядра с запасом: (17 ядер + 3.4 ядра) × 1.5 = 20.4 ядер × 1.5 = 30.6 ядер
```

После учета системных и служебных потребностей, а также добавления запаса для отказоустойчивости, получаем следующие итоговые ресурсы:
```
Итоговая ОЗУ: 54.45 ГБ
Итоговое количество ядер: 30.6 ядер
```

#### 5. Рекомендуемые ресурсы на ноду

Т.к. выше мы определились, что будем использовать 3 воркер ноды, то рекомендуемые ресурсы на ноду:
```
ОЗУ: 54.45 ГБ / 3 ≈ 18.15 ГБ
Процессорные ядра: 30.6 ядер / 3 ≈ 10.2 ядра
```

Таким образом, каждая нода должна быть оснащена примерно 18-19 ГБ ОЗУ и 10-11 процессорными ядрами.

## Расчет ресурсов Control plane

Основные компоненты Control Plane и их оптимальные ресурсы

```
Минимальное количество копий каждого компонента для обеспечения отказоустойчивости и нагрузки: 3
```

**kube-apiserver**:\
Основной компонент, который обслуживает все запросы к кластеру.
```
Рекомендуемые ресурсы: 1 ядро, 1 ГБ ОЗУ на каждую копию.
Количество копий: 3
```

**etcd**:\
 Хранит всю конфигурацию и состояние кластера.
```
Рекомендуемые ресурсы: 2 ядра, 4 ГБ ОЗУ на каждую копию.
Количество копий: 3
```

**kube-scheduler**:\
Отвечает за распределение подов на ноды.
```
Рекомендуемые ресурсы: 0.5 ядра, 512 МБ ОЗУ на каждую копию.
Количество копий: 3
```

**kube-controller-manager**:\
Управляет различными контроллерами, которые поддерживают состояние кластера.
```
Рекомендуемые ресурсы: 0.5 ядра, 512 МБ ОЗУ на каждую копию.
Количество копий: 3
```

**cloud-controller-manager**:\
Устанавливается, если используются облачные сервисы.
```
Рекомендуемые ресурсы: 0.25 ядра, 256 МБ ОЗУ на каждую копию.
Количество копий: 3
```

### Перерасчет ресурсов для каждого компонента
kube-apiserver:

    ОЗУ: 3 × 1 ГБ = 3 ГБ
    Ядра: 3 × 1 ядро = 3 ядра

etcd:

    ОЗУ: 3 × 4 ГБ = 12 ГБ
    Ядра: 3 × 2 ядра = 6 ядер

kube-scheduler:

    ОЗУ: 3 × 512 МБ = 1.5 ГБ
    Ядра: 3 × 0.5 ядра = 1.5 ядра

kube-controller-manager:

    ОЗУ: 3 × 512 МБ = 1.5 ГБ
    Ядра: 3 × 0.5 ядра = 1.5 ядра

cloud-controller-manager:

    ОЗУ: 3 × 256 МБ = 768 МБ ≈ 1 ГБ
    Ядра: 3 × 0.25 ядра = 0.75 ядра

### Итоговые ресурсы для Control Plane
Без учета запаса:
```
ОЗУ: 3 + 12 + 1.5 + 1.5 + 1 ГБ = 19 ГБ
Ядра: 3 + 6 + 1.5 + 1.5 + 0.75 = 12.75 ядер
```

Для учета отказоустойчивости умножим ресурсы на коэффициент N/(N-1) (описание см. выше), где N число нод равное 3:
```
ОЗУ с запасом: 19 ГБ × 1.5 = 28.5 ГБ ≈ 29 ГБ
Ядра с запасом: 12.75 ядер × 1.5 = 19.125 ядер ≈ 19 ядер
```
 
Добавим 20% на системные службы и другие необходимые сервисы на нодах:
```
Служебные ресурсы для ОЗУ: 29 ГБ × 0.2 = 5.8 ГБ ≈ 6 ГБ
Служебные ресурсы для ядер: 19 ядер × 0.2 = 3.8 ядра ≈ 4 ядра
```

Итоговые ресурсы для Control Plane с учетом служебных ресурсов
```
Итоговая ОЗУ: 29 ГБ + 6 ГБ = 35 ГБ
Итоговое количество ядер: 19 ядер + 4 ядра = 23 ядра
```

### Количество и характеристики нод Control Plane

    Количество нод: 3
    Рекомендуемые ресурсы на ноду:
        ОЗУ: 35 ГБ / 3 ≈ 11.67 ГБ ≈ 12 ГБ
        Процессорные ядра: 23 ядер / 3 ≈ 7.67 ядра ≈ 8 ядер

Этот расчет учитывает стандартные практики и требования для Kubernetes Control Plane, обеспечивая достаточную отказоустойчивость и стабильность. Это обеспечивает запас для работы системных служб и масштабируемость в случае увеличения нагрузки.

### Обоснование оптимальности расчетов

Отказоустойчивость и балансировка нагрузки:
- Использование 3 нод для Control Plane является стандартом для обеспечения отказоустойчивости, позволяя выдерживать выход из строя одной ноды без потери функциональности кластера.
- Каждая нода может принимать на себя нагрузку других нод в случае их выхода из строя, обеспечивая непрерывную работу.

Резервирование и запас ресурсов:
- Увеличение ресурсов на 50% позволяет покрыть пиковые нагрузки и дает возможность для проведения обслуживания без остановки всех компонентов.
- Добавление 20% ресурсов для служебных задач гарантирует, что системные службы не будут конкурировать с основными процессами Control Plane за ресурсы, что важно для стабильной работы.

Адекватное распределение ресурсов:
- Распределение 12 ГБ ОЗУ и 8 ядер на ноду гарантирует, что каждое ядро и каждый гигабайт оперативной памяти будут использованы эффективно, без избыточных простоев или перегрузок.
- Ресурсы на каждую ноду учтены с запасом для масштабируемости и будущих изменений в конфигурации или объеме работы.

Обеспечение производительности:
- Выбор ресурсов для ключевых компонентов (API-сервер, etcd, контроллеры и т.д.) основан на типичных нагрузках и требованиях, что позволяет обеспечить их стабильную работу и быстрое реагирование на запросы.
- Оптимизация использования ресурсов позволяет минимизировать задержки и максимизировать пропускную способность кластера.

### Проверка и настройка

Несмотря на теоретическую оптимальность расчетов, важно провести практическую проверку в реальных условиях эксплуатации:

Мониторинг и сбор метрик:
- Запустить кластер с рассчитанными ресурсами и использовать инструменты мониторинга (например, Prometheus и Grafana) для сбора данных о потреблении ресурсов и производительности.
- Оценить, достаточно ли ресурсов для всех процессов и нет ли узких мест.

Тестирование на отказоустойчивость:
- Провести тесты на отказ нод, чтобы убедиться, что кластер может выдержать выход из строя одной или нескольких нод и продолжать работать.
- Проверить, как распределяется нагрузка в случае отказа и сколько времени занимает восстановление.

Адаптация под конкретные условия:
- В зависимости от результатов мониторинга и тестирования, возможно, потребуется скорректировать количество ресурсов или нод для оптимальной работы в конкретных условиях.
- Необходимо учитывать особенности используемых приложений, возможные будущие увеличения нагрузки и другие специфические факторы.

Таким образом, предложенные ресурсы для Control Plane являются оптимальными на основе общепринятых практик и теоретических расчетов. Однако, для достижения максимальной эффективности рекомендуется провести практическое тестирование и корректировку на основе реальных условий эксплуатации.

## Итого:
```
Control plane: 3 ноды с 12 ГБ ОЗУ и 8 ядрами на каждую ноду.
Worker nodes: 3 ноды с 18-19 ГБ ОЗУ и 10-11 ядрами на каждую ноду.
```
